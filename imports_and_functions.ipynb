{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions needed to import and structure Crunchbase data\n",
    "# Written by David Chataway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import re\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import math\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crunchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crunchbase_export(filename, queries):\n",
    "    # Filename is a string including the .csv \n",
    "    # queries is a dictionary of the column:filter\n",
    "    \n",
    "    ##################################################\n",
    "    # READ \n",
    "    ##################################################\n",
    "    # Read csv, then structure and prepare it based on the query provided\n",
    "    df = pd.read_csv(\"Crunchbase/exports/\"+filename)\n",
    "\n",
    "    ##################################################\n",
    "    # CONFIGURE AND QUERY \n",
    "    ##################################################\n",
    "    # Date configuration - extract date of date of data extraction \n",
    "    match = re.findall(r'[0-9]+-[0-9]+-[0-9]{4}', filename)\n",
    "    format = \"%m-%d-%Y\"\n",
    "    dt_object = datetime.strptime(match[0], format)\n",
    "\n",
    "    # Convert into a date range with T-5 years\n",
    "    start_date = dt_object - relativedelta(years=5)\n",
    "\n",
    "    # Create date range for later\n",
    "    dates = pd.date_range(start=start_date,end=dt_object)\n",
    "    df_day = pd.DataFrame({'date':dates})\n",
    "\n",
    "    # Query and filter the dataframe\n",
    "    df_queried = df.copy(deep=True)\n",
    "    df_queried['Announced Date'] = pd.to_datetime(df_queried['Announced Date'])\n",
    "    if len(queries) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for key in queries.keys():\n",
    "            df_queried = df_queried[df_queried[key] == queries[key]]\n",
    "    \n",
    "    ##################################################\n",
    "    # TRANSFORM TO DATE \n",
    "    ##################################################    \n",
    "    \n",
    "    # USER DEFINED COLUMNS TO AGGREGATE\n",
    "    y_cols = {'Total Funding Amount Currency (in USD)':[], 'Money Raised Currency (in USD)':[], 'Pre-Money Valuation Currency (in USD)':[]}\n",
    "    # Count number of founding rounds\n",
    "    ls_count = []\n",
    "\n",
    "    # Generate aggregated dataframe by day\n",
    "    cols = list(y_cols.keys())\n",
    "    cols.append('Announced Date')\n",
    "    df_grouped = df_queried[cols].groupby('Announced Date', as_index=False).sum()\n",
    "\n",
    "    # Loop once through dates\n",
    "    for i in dates:\n",
    "        count = 0\n",
    "        # Count the entries\n",
    "        for j in df_queried['Announced Date']:\n",
    "            if (j >= i and j < (i+timedelta(days=1))):\n",
    "                count += 1\n",
    "        ls_count.append(count)\n",
    "\n",
    "        # Get aggregate sums by date\n",
    "        if df_grouped.loc[df_grouped['Announced Date'] == i].empty:\n",
    "            for key in y_cols.keys():\n",
    "                y_cols[key].append(0)\n",
    "        else:\n",
    "            for key in y_cols.keys():\n",
    "                y_cols[key].append(float(df_grouped[key][df_grouped['Announced Date'] == i]))\n",
    "\n",
    "    # Add the information to the df_day dataframe  \n",
    "    df_day['count'] = ls_count\n",
    "    for key in y_cols.keys():\n",
    "        df_day[key] = y_cols[key]\n",
    "\n",
    "    # GROUPING BY WEEK\n",
    "    #df_day['Start_of_week'] = pd.to_datetime(df_day['date']).to_period('W').start_time\n",
    "    df_week = df_day.groupby(pd.Grouper(key='date', axis=0, \n",
    "                          freq='W')).sum().reset_index()\n",
    "    #df_week['week'] =  df_week['date'].dt.week\n",
    "    #df_week['year'] =  df_week['date'].dt.year\n",
    "    \n",
    "    # Returning the Start of the week for each date\n",
    "    ls_start_of_week = []\n",
    "    for date in df_week['date']:\n",
    "        ls_start_of_week.append(date.to_period('W').start_time)\n",
    "    df_week['Start_of_week'] = ls_start_of_week\n",
    "        \n",
    "    return df, df_queried, df_day, df_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "      <th>Total Funding Amount Currency (in USD)</th>\n",
       "      <th>Money Raised Currency (in USD)</th>\n",
       "      <th>Pre-Money Valuation Currency (in USD)</th>\n",
       "      <th>Start_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4480000.0</td>\n",
       "      <td>1230000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>2023-02-12</td>\n",
       "      <td>1</td>\n",
       "      <td>4900000.0</td>\n",
       "      <td>4500000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>4</td>\n",
       "      <td>19075000.0</td>\n",
       "      <td>19075000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>2023-02-26</td>\n",
       "      <td>5</td>\n",
       "      <td>14600000.0</td>\n",
       "      <td>12800000.0</td>\n",
       "      <td>13000000.0</td>\n",
       "      <td>2023-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2023-03-05</td>\n",
       "      <td>3</td>\n",
       "      <td>7400000.0</td>\n",
       "      <td>7400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>2</td>\n",
       "      <td>28000000.0</td>\n",
       "      <td>28000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-03-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  count  Total Funding Amount Currency (in USD)  \\\n",
       "0   2018-03-11      0                                     0.0   \n",
       "1   2018-03-18      0                                     0.0   \n",
       "2   2018-03-25      1                              15000000.0   \n",
       "3   2018-04-01      1                               4480000.0   \n",
       "4   2018-04-08      0                                     0.0   \n",
       "..         ...    ...                                     ...   \n",
       "257 2023-02-12      1                               4900000.0   \n",
       "258 2023-02-19      4                              19075000.0   \n",
       "259 2023-02-26      5                              14600000.0   \n",
       "260 2023-03-05      3                               7400000.0   \n",
       "261 2023-03-12      2                              28000000.0   \n",
       "\n",
       "     Money Raised Currency (in USD)  Pre-Money Valuation Currency (in USD)  \\\n",
       "0                               0.0                                    0.0   \n",
       "1                               0.0                                    0.0   \n",
       "2                        15000000.0                                    0.0   \n",
       "3                         1230000.0                                    0.0   \n",
       "4                               0.0                                    0.0   \n",
       "..                              ...                                    ...   \n",
       "257                       4500000.0                                    0.0   \n",
       "258                      19075000.0                                    0.0   \n",
       "259                      12800000.0                             13000000.0   \n",
       "260                       7400000.0                                    0.0   \n",
       "261                      28000000.0                                    0.0   \n",
       "\n",
       "    Start_of_week  \n",
       "0      2018-03-05  \n",
       "1      2018-03-12  \n",
       "2      2018-03-19  \n",
       "3      2018-03-26  \n",
       "4      2018-04-02  \n",
       "..            ...  \n",
       "257    2023-02-06  \n",
       "258    2023-02-13  \n",
       "259    2023-02-20  \n",
       "260    2023-02-27  \n",
       "261    2023-03-06  \n",
       "\n",
       "[262 rows x 6 columns]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "filename = \"web-3-search-3-11-2023.csv\"\n",
    "queries = {\n",
    "    'Equity Only Funding':'Yes'\n",
    "    #,'Funding Type':'Seed'\n",
    "}\n",
    "df_raw, df_queried, df_day, df_week = read_crunchbase_export(filename, queries)\n",
    "df_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_google_trends_export(filename_google):\n",
    "    # Filename is a string including the .csv \n",
    "\n",
    "    ##################################################\n",
    "    # READ \n",
    "    ##################################################\n",
    "    # read csv and extract metadata of the search contained in the first cell\n",
    "    df_google = pd.read_csv(\"Google_Trends/exports/\"+filename_google)\n",
    "    \n",
    "    if df_google.iloc[1][0] == 'Week':\n",
    "        Google_search_meta = df_google.iloc[1]\n",
    "        df_google = df_google.drop([0], axis = 0)\n",
    "    else:\n",
    "        Google_search_meta = df_google.iloc[0]\n",
    "\n",
    "    ##################################################\n",
    "    # CONFIGURE \n",
    "    ##################################################\n",
    "\n",
    "    # Structure the dataframe\n",
    "    try:\n",
    "        df_google = df_google.drop(['Week'], axis = 0)\n",
    "    except:\n",
    "        df_google = df_google.drop([1], axis = 0)\n",
    "        df_google = df_google.set_index('Category: All categories')\n",
    "        \n",
    "    # Cast the Week range as a datetime64 type\n",
    "    # df_google['date'] = df_google.index.astype('datetime64[ns]')\n",
    "    #df_google['week'] =  pd.to_datetime(df_google.index).week\n",
    "    #df_google['year'] =  pd.to_datetime(df_google.index).year\n",
    "    df_google['Start_of_week'] = pd.to_datetime(df_google.index).to_period('W').start_time\n",
    "\n",
    "    return df_google #, Google_search_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category: All categories</th>\n",
       "      <th>Start_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023-02-05</td>\n",
       "      <td>40</td>\n",
       "      <td>2023-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023-02-12</td>\n",
       "      <td>36</td>\n",
       "      <td>2023-02-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>37</td>\n",
       "      <td>2023-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023-02-26</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2023-03-05</td>\n",
       "      <td>37</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Category: All categories Start_of_week\n",
       "2018-03-18                        5    2018-03-12\n",
       "2018-03-25                        5    2018-03-19\n",
       "2018-04-01                        5    2018-03-26\n",
       "2018-04-08                        4    2018-04-02\n",
       "2018-04-15                        4    2018-04-09\n",
       "...                             ...           ...\n",
       "2023-02-05                       40    2023-01-30\n",
       "2023-02-12                       36    2023-02-06\n",
       "2023-02-19                       37    2023-02-13\n",
       "2023-02-26                       43    2023-02-20\n",
       "2023-03-05                       37    2023-02-27\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "filename_google = \"Web3-Google-3-11-2023.csv\"\n",
    "df_google = read_google_trends_export(filename_google)\n",
    "df_google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "# PULL HASHTAGS OF TOP VCs\n",
    "\n",
    "# For instance, Sequoia Capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Crunchbase and Google Datasets\n",
    "##### By week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weekly(df_google, df_crunchbase):\n",
    "\n",
    "    # Requires two dataframes, where\n",
    "    # [0] = Google df\n",
    "    # [1] = Crunchbase df\n",
    "        \n",
    "    df_combined_week = pd.DataFrame()\n",
    "    #df_combined_week = df_google.merge(df_crunchbase, on = ['week','year'], how = 'inner')\n",
    "    df_combined_week = df_google.merge(df_crunchbase, on = ['Start_of_week'], how = 'inner')\n",
    "    \n",
    "    return df_combined_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category: All categories</th>\n",
       "      <th>Start_of_week</th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "      <th>Total Funding Amount Currency (in USD)</th>\n",
       "      <th>Money Raised Currency (in USD)</th>\n",
       "      <th>Pre-Money Valuation Currency (in USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-03-12</td>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-03-19</td>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>15000000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4480000.0</td>\n",
       "      <td>1230000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>2018-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-04-09</td>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>40</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>2023-02-05</td>\n",
       "      <td>3</td>\n",
       "      <td>36100000.0</td>\n",
       "      <td>24000000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>36</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>2023-02-12</td>\n",
       "      <td>1</td>\n",
       "      <td>4900000.0</td>\n",
       "      <td>4500000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>37</td>\n",
       "      <td>2023-02-13</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>4</td>\n",
       "      <td>19075000.0</td>\n",
       "      <td>19075000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>43</td>\n",
       "      <td>2023-02-20</td>\n",
       "      <td>2023-02-26</td>\n",
       "      <td>5</td>\n",
       "      <td>14600000.0</td>\n",
       "      <td>12800000.0</td>\n",
       "      <td>13000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>37</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>2023-03-05</td>\n",
       "      <td>3</td>\n",
       "      <td>7400000.0</td>\n",
       "      <td>7400000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Category: All categories Start_of_week       date  count  \\\n",
       "0                          5    2018-03-12 2018-03-18      0   \n",
       "1                          5    2018-03-19 2018-03-25      1   \n",
       "2                          5    2018-03-26 2018-04-01      1   \n",
       "3                          4    2018-04-02 2018-04-08      0   \n",
       "4                          4    2018-04-09 2018-04-15      0   \n",
       "..                       ...           ...        ...    ...   \n",
       "255                       40    2023-01-30 2023-02-05      3   \n",
       "256                       36    2023-02-06 2023-02-12      1   \n",
       "257                       37    2023-02-13 2023-02-19      4   \n",
       "258                       43    2023-02-20 2023-02-26      5   \n",
       "259                       37    2023-02-27 2023-03-05      3   \n",
       "\n",
       "     Total Funding Amount Currency (in USD)  Money Raised Currency (in USD)  \\\n",
       "0                                       0.0                             0.0   \n",
       "1                                15000000.0                      15000000.0   \n",
       "2                                 4480000.0                       1230000.0   \n",
       "3                                       0.0                             0.0   \n",
       "4                                       0.0                             0.0   \n",
       "..                                      ...                             ...   \n",
       "255                              36100000.0                      24000000.0   \n",
       "256                               4900000.0                       4500000.0   \n",
       "257                              19075000.0                      19075000.0   \n",
       "258                              14600000.0                      12800000.0   \n",
       "259                               7400000.0                       7400000.0   \n",
       "\n",
       "     Pre-Money Valuation Currency (in USD)  \n",
       "0                                      0.0  \n",
       "1                                      0.0  \n",
       "2                                      0.0  \n",
       "3                                      0.0  \n",
       "4                                      0.0  \n",
       "..                                     ...  \n",
       "255                                    0.0  \n",
       "256                                    0.0  \n",
       "257                                    0.0  \n",
       "258                             13000000.0  \n",
       "259                                    0.0  \n",
       "\n",
       "[260 rows x 7 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "df_combined_week = merge_weekly(df_google, df_week)\n",
    "df_combined_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(dfs, window_size = 3):\n",
    "    dfs_return = {}\n",
    "    # Requires dictionary of dfs\n",
    "    # window is an int that describes the size of the moving average window\n",
    "    \n",
    "    ##################################################\n",
    "    # STANDARDIZE COLUMNS TO 100\n",
    "    ##################################################  \n",
    "    \n",
    "    # Make $ columns relative\n",
    "    y_cols = {'Total Funding Amount Currency (in USD)':[], 'Money Raised Currency (in USD)':[], 'Pre-Money Valuation Currency (in USD)':[]}\n",
    "\n",
    "    # Loop through the dataframes\n",
    "    for key in dfs.keys(): \n",
    "        \n",
    "        # Copy the dataframe\n",
    "        df_return = dfs[key].copy(deep=True)\n",
    "\n",
    "        post_process_cols = []\n",
    "\n",
    "        for key_cols in y_cols.keys():\n",
    "            col_name = key_cols+' %'\n",
    "            post_process_cols.append(key_cols)\n",
    "            post_process_cols.append(col_name)\n",
    "            df_return[col_name] = df_return[key_cols] / max(df_return[key_cols]) * 100\n",
    "\n",
    "        df_return['count %'] = df_return['count'] / max(df_return['count']) * 100\n",
    "        post_process_cols.append('count')\n",
    "        post_process_cols.append('count %')\n",
    "\n",
    "        # Make Google Trends info a column\n",
    "        try:\n",
    "            df_return['Google Trends'] = df_return['Category: All categories']\n",
    "        except:\n",
    "            df_return['Google Trends'] = df_return['Unnamed: 1']\n",
    "        # Correct for <1\n",
    "        df_return.loc[df_return['Google Trends'] == '<1', 'Google Trends'] = 0\n",
    "        # Cast as float\n",
    "        df_return['Google Trends'] = df_return['Google Trends'].astype(float)\n",
    "        post_process_cols.append('Google Trends')\n",
    "\n",
    "        ##################################################\n",
    "        # SMOOTH VIA MOVING AVERAGE \n",
    "        ##################################################  \n",
    "        \n",
    "        # simple moving averages using pandas; don't need a spline because we aren't looking to fit a smoother to the data\n",
    "        for col in post_process_cols:\n",
    "            col_name = col + '-Rolling'\n",
    "\n",
    "            # Get the window of series ... # of observations of specified window size\n",
    "            windows = df_return[col].rolling(window_size, center = True)\n",
    "\n",
    "            # Create a series of moving averages of each window\n",
    "            moving_averages = windows.mean()\n",
    "\n",
    "            # Correct_null values\n",
    "            if window_size == 3:\n",
    "                moving_averages[0] = df_return[col][0]\n",
    "                moving_averages[len(moving_averages)-1] = df_return[col][len(moving_averages)-1]\n",
    "            elif window_size == 5: \n",
    "                moving_averages[0] = df_return[col][0]\n",
    "                moving_averages[1] = df_return[col][1]\n",
    "                moving_averages[len(moving_averages)-1] = df_return[col][len(moving_averages)-1]\n",
    "                moving_averages[len(moving_averages)-2] = df_return[col][len(moving_averages)-2]\n",
    "            else: \n",
    "                raise ValueError('Please only use window size of 3 or 5 or update the code')\n",
    "                \n",
    "            # Assign\n",
    "            df_return[col_name] = moving_averages\n",
    "            \n",
    "            # Re-standardize based on smoothed columns (max = 100)\n",
    "            if ' %' in col_name:\n",
    "                pass\n",
    "            else:\n",
    "                df_return[str(col_name + ' %')] = df_return[col_name] / max(df_return[col_name]) * 100\n",
    "        \n",
    "        # Append to dictionary\n",
    "        dfs_return[key] = df_return\n",
    "        \n",
    "    return dfs_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of Peak Hype and Peak Funding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: The local maxima would be any x point which has a higher y value than either of its left and right neighbors. To eliminate noise, you could put in some kind of tolerance threshold (ex. x point must have higher y value than n of its neighbors). And specify a distance away threshold.\n",
    "\n",
    "Other Methods for Peak Finding https://www.originlab.com/index.aspx?go=Products/Origin/DataAnalysis/PeakAnalysis#:~:text=Peak%20Finding%2FDetermination%201%20Savitzky-Golay%20smoothing%20on%20the%20spectrum,Derivative%20%28for%20hidden%20peaks%29%20Fourier%20Self%20Deconvolution%20PRO\n",
    "\n",
    "1. Savitzky-Golay smoothing on the spectrum before peak finding\n",
    "2. 6 methods for finding peaks: i) Local Maximum, ii) Window Search, iii) First Derivative, iv) Second Derivative (for hidden peaks), v) Residual after 1st Derivative (for hidden peaks), vi) Fourier Self Deconvolution PRO\n",
    "3. After finding peaks, you can filter out unwanted peaks: i) By peak height, ii) By number of peaks, iii) Label peaks with X, Y values or row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks (dfs, min_height = 0.6, min_threshold = 0.00, min_spacing = 2, rolling = True):\n",
    "    # Takes in the dictionary of dataframes and outputs a dictionary with the peaks\n",
    "    # Min height is the necessary absolute height to be considered a peak\n",
    "    # Threshold is necessary relative height difference (vs neighbors) to be considered a subsequent peak\n",
    "    # Min spacing is the minimum time period (i.e. weeks) mandated between peaks\n",
    "    \n",
    "    dfs_peaks = {}\n",
    "\n",
    "    # Loop through the dataframes\n",
    "    for key in dfs.keys(): \n",
    "        peaks_dict = {}\n",
    "\n",
    "        if rolling:\n",
    "            cols = ['Google Trends-Rolling',\n",
    "                    'Google Trends-Rolling %',\n",
    "                    'count-Rolling', 'count-Rolling %', 'count %-Rolling', \n",
    "                    'Total Funding Amount Currency (in USD)-Rolling',\n",
    "                    'Total Funding Amount Currency (in USD)-Rolling %',\n",
    "                    'Total Funding Amount Currency (in USD) %-Rolling',\n",
    "                    'Money Raised Currency (in USD)-Rolling',\n",
    "                    'Money Raised Currency (in USD)-Rolling %',\n",
    "                    'Money Raised Currency (in USD) %-Rolling',\n",
    "                    'Pre-Money Valuation Currency (in USD)-Rolling',\n",
    "                    'Pre-Money Valuation Currency (in USD)-Rolling %',\n",
    "                    'Pre-Money Valuation Currency (in USD) %-Rolling']\n",
    "        else:\n",
    "            cols = ['Google Trends',\n",
    "                    'count',\n",
    "                    'Total Funding Amount Currency (in USD)', \n",
    "                    'Money Raised Currency (in USD)', \n",
    "                    'Pre-Money Valuation Currency (in USD)'\n",
    "                    'Total Funding Amount Currency (in USD) %',\n",
    "                    'Money Raised Currency (in USD) %',\n",
    "                    'Pre-Money Valuation Currency (in USD) %', \n",
    "                    'count %']\n",
    "\n",
    "        ##################################################\n",
    "        # FIND PEAK HYPE (GOOGLE SEARCHES) \n",
    "        ################################################## \n",
    "        \n",
    "        # Finds the peak of Google Trends (col no 0)\n",
    "        peaks_dict['Hype'] = dfs[key]['date'].iloc[dfs[key][cols[0]].idxmax()]\n",
    "        # Break ties with the earlier date\n",
    "        if isinstance(peaks_dict['Hype'], list):\n",
    "            peaks_dict['Hype'] = min(peaks_dict['Hype'])\n",
    "        \n",
    "        ##################################################\n",
    "        # FIND PEAKS OF FUNDING \n",
    "        ##################################################        \n",
    "        \n",
    "        for col in cols:\n",
    "            # https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.argrelextrema.html\n",
    "            # https://stackoverflow.com/questions/35282456/find-local-maximums-in-numpy-array\n",
    "            # Statistical methods vs analytical ones?\n",
    "            \n",
    "            # For each column, return array of peaks\n",
    "            peaks, properties = find_peaks(dfs[key][col], distance= min_spacing, height=min_height*max(dfs[key][col]), threshold = min_threshold*max(dfs[key][col]))\n",
    "            if len(peaks) == 0:\n",
    "                peaks = np.array([0])\n",
    "            peaks_dict[col + '_array'] = peaks\n",
    "            peaks_dict[col + '_array_properties'] = properties\n",
    "            \n",
    "            # Get the complete width (~3 sigma) at peaks \n",
    "            full_widths = peak_widths(dfs[key][col], peaks, rel_height=1)\n",
    "            peaks_dict[col + '_full_widths'] = full_widths\n",
    "            peaks_dict[col + '_array_widths'] = full_widths[0] \n",
    "            \n",
    "            # Find the earliest peak\n",
    "            peaks_dict[col] = min(peaks)\n",
    "            peaks_dict[col + '_date'] = dfs[key]['date'][min(peaks)]\n",
    "            peaks_dict[col + '_width'] = full_widths[0][np.where(peaks==min(peaks))[0][0]] \n",
    "            \n",
    "            # Find the highest peak\n",
    "            peaks_dict[col + '_max'] = dfs[key][col].idxmax()\n",
    "            # Break ties with the earlier date\n",
    "            if isinstance(peaks_dict[col + '_max'], list):\n",
    "                peaks_dict[col + '_max'] = min(peaks_dict[col + '_max'])\n",
    "            elif math.isnan(peaks_dict[col + '_max']):\n",
    "                peaks_dict[col + '_max'] = 0\n",
    "            peaks_dict[col + '_max_date'] = dfs[key]['date'].iloc[peaks_dict[col + '_max']]\n",
    "            \n",
    "        # Assign to DataFrame\n",
    "        dfs_peaks[key] = peaks_dict\n",
    "        \n",
    "    return dfs_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate a Hype Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_single_technology (key, x_col, y_1_col, y_2_col, saving_plots, standardize):\n",
    "\n",
    "    x = df_mains[key][x_col]\n",
    "    y_smooth = df_mains[key][y_1_col]\n",
    "    z_smooth = df_mains[key][y_2_col]\n",
    "    if standardize:\n",
    "        y_smooth = y_smooth * 100 / max(y_smooth)\n",
    "        z_smooth = z_smooth * 100 / max(z_smooth)\n",
    "\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    sns.set_style(\"whitegrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    plt.plot(x,y_smooth, color = 'g', label = y_1_col)\n",
    "\n",
    "    plt.plot(x,z_smooth, color='b', label = y_2_col)\n",
    "\n",
    "    plt.title(key + ' - Early Stage Funding and Google Trends by Week')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,100])\n",
    "    if saving_plots:\n",
    "        plt.savefig(\"Plots/Time-Series/\" + key + '_standardized='+str(standardize)+'.png', dpi = 1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_single_technology_peaks (key, x_col, y_1_col, y_2_col, saving_plots, standardize):\n",
    "\n",
    "    x = df_mains[key][x_col]\n",
    "    y_smooth = df_mains[key][y_1_col]\n",
    "    z_smooth = df_mains[key][y_2_col]\n",
    "    if standardize:\n",
    "        y_smooth = y_smooth * 100 / max(y_smooth)\n",
    "        z_smooth = z_smooth * 100 / max(z_smooth)\n",
    "\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    sns.set_style(\"whitegrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    plt.plot(x,y_smooth, color = 'g', label = y_1_col)\n",
    "\n",
    "    plt.plot(x,z_smooth, color='b', label = y_2_col)\n",
    "\n",
    "    #PLOT PEAKS\n",
    "    plt.axvline(x = df_peaks[key]['Hype'], label = 'Moment Peak Hype', color=\"C4\")\n",
    "    plt.axvline(x = df_peaks[key][y_2_col +'_date'], label = 'Earliest Moment of Peak Hype', color=\"C4\", ls = '--')\n",
    "    plt.axvline(x = df_peaks[key][y_1_col +'_date'], label = 'Earliest Moment of Peak Funding', color=\"black\")\n",
    "\n",
    "    peak_indices = df_peaks[key][y_1_col +'_array']\n",
    "    plt.plot(x[peak_indices], y_smooth[peak_indices], \"x\", label = 'Peaks in Funding', color = 'black', markersize=16)\n",
    "\n",
    "    plt.title(key + ' - Early Stage Funding and Google Trends by Week - with Peaks')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,100])\n",
    "    if saving_plots:\n",
    "        plt.savefig(\"Plots/Time-Series/\" + key + '_smooth_peaks.png', dpi = 1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_multi_technology_peaks (keys, x_col, y_1_col, y_2_col, saving_plots, standardize):\n",
    "    \n",
    "    df_return = pd.DataFrame(columns = [\"Key\", \"Title\", \"Value\"])\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    sns.set_style(\"whitegrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=len(keys), ncols=1, figsize=(24, 56))\n",
    "    fig.suptitle('Early Stage Funding and Google Trends\\nby Week (with Peaks shown)', fontsize=32)\n",
    "\n",
    "    for i in range(0,len(keys)):\n",
    "        key = keys[i]\n",
    "        \n",
    "        x = df_mains[key][x_col]\n",
    "        y_smooth = df_mains[key][y_1_col]\n",
    "        z_smooth = df_mains[key][y_2_col]\n",
    "        if standardize:\n",
    "            y_smooth = y_smooth * 100 / max(y_smooth)\n",
    "            z_smooth = z_smooth * 100 / max(z_smooth)\n",
    "\n",
    "        #Plot Google trends\n",
    "        axs[i].plot(x,z_smooth, color='b', label = y_2_col)\n",
    "        axs[i].axvline(x = df_peaks[key]['Hype'], label = 'Moment of Peak Hype', color=\"darkblue\")\n",
    "        axs[i].axvline(x = df_peaks[key][y_2_col +'_date'], label = 'Earliest Moment of Peak Hype', color=\"darkblue\", ls = '--')\n",
    "        axs[i].axvspan(df_peaks[key]['Hype'], df_peaks[key][y_2_col +'_date'], alpha=0.3, color='lightsteelblue', label = 'Hype Innovation Trigger Range')\n",
    "    \n",
    "        # Plot funding\n",
    "        axs[i].plot(x,y_smooth, color = 'g', label = y_1_col)\n",
    "        axs[i].axvline(x = df_peaks[key][y_1_col +'_date'], label = 'Earliest Moment of Peak Funding', color=\"black\")\n",
    "        axs[i].axvspan(df_peaks[key][y_1_col +'_date'], df_peaks[key][y_1_col +'_max_date'], alpha=0.3, color='palegreen', label = 'Innovation Funding Range')\n",
    "        peak_indices = df_peaks[key][y_1_col +'_array']\n",
    "        axs[i].plot(x[peak_indices], y_smooth[peak_indices], \"x\", label = 'Peaks in Funding', color = 'black', markersize=12)\n",
    "\n",
    "        axs[i].set_title(key)\n",
    "        #axs[i].legend()\n",
    "        axs[i].set_ylim([0,100])\n",
    "        \n",
    "        # Add to return dataframe\n",
    "        df_return = df_return.append({'Key': key, 'Title': 'Earliest Moment of Peak Hype', 'Value': df_peaks[key][y_2_col +'_date']}, ignore_index=True)\n",
    "        df_return = df_return.append({'Key': key, 'Title': 'Moment of Peak Hype', 'Value': df_peaks[key]['Hype']}, ignore_index=True)\n",
    "        df_return = df_return.append({'Key': key, 'Title': 'Earliest Moment of Peak Funding', 'Value': df_peaks[key][y_1_col +'_date']}, ignore_index=True)\n",
    "        df_return = df_return.append({'Key': key, 'Title': 'Moment of Peak Funding', 'Value': df_peaks[key][y_1_col +'_max_date']}, ignore_index=True)\n",
    "        ### KEY - TIME DELTA defined by EARLIEST FUNDING - EARLIEST HYPE moments\n",
    "        df_return = df_return.append({'Key': key, 'Title': 'Time Delta (Days)', 'Value': (df_peaks[key][y_1_col +'_date'] - df_peaks[key][y_2_col +'_date']).days}, ignore_index=True)\n",
    "        df_return = df_return.append({'Key': key, 'Title': 'Time Delta (Days) - Max Amounts', 'Value': (df_peaks[key][y_1_col +'_max_date']-df_peaks[key]['Hype']).days}, ignore_index=True)\n",
    "\n",
    "    # Single legend\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels) #, loc='upper center')\n",
    "\n",
    "    # Fix formatting\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.93)\n",
    "\n",
    "    if saving_plots:    \n",
    "        plt.savefig(\"Plots/Time-Series/All_Technologies_with_Peaks.png\", dpi = 300)\n",
    "        df_return.to_csv('Data/time-series-results.csv')\n",
    "    \n",
    "    plt.show()\n",
    "    return plt, fig, df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_summary (keys, df_return):\n",
    "    x = list(df_return[df_return['Title'] == 'Time Delta (Days)']['Value'] / 365)\n",
    "    y = list(df_return[df_return['Title'] == 'Earliest Moment of Peak Hype']['Value'])\n",
    "\n",
    "    sns.set(rc={'figure.figsize': (16, 8)})\n",
    "    # plotting scatter plot\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.scatter(x, y, label = 'Data') #sns.scatterplot(x, y)\n",
    "    plt.ylim(max(y)+timedelta(days=180), min(y)-timedelta(days=180)) #show year in descending order\n",
    "\n",
    "    # Mean and STD\n",
    "    average = np.mean(df_return[df_return['Title'] == 'Time Delta (Days)']['Value']) \n",
    "    print('The average time difference between hype and funding cycles is: ' + str(average))\n",
    "    st_dev = np.std(df_return[df_return['Title'] == 'Time Delta (Days)']['Value'])\n",
    "    print('The standard deviation is: ' + str(st_dev))\n",
    "    \n",
    "    # Plot helper lines\n",
    "    plt.axvline(x = (average/365), ls = '--', label = 'Average Funding Time Lag')\n",
    "    plt.axvline(x = 0, ls = '-', color = 'black')\n",
    "    plt.axvspan((average-st_dev)/365, (average+st_dev)/365 , alpha=0.2, color='lightsteelblue', label = 'Standard Deviation of the Mean')\n",
    "\n",
    "    # Loop for annotation of all points\n",
    "    for i in range(len(x)):\n",
    "        plt.annotate(keys[i], (x[i]+0.02, y[i]))\n",
    "    # annotation of the third point\n",
    "    # plt.text(2,4.2,\"third\")    \n",
    "    \n",
    "    plt.ylabel('Date Corresponding to Start of Hype Cycle', fontsize = 16)\n",
    "    plt.xlabel('Time Difference (Years) Between Start of Hype Cycle and Start of Funding Cycle', fontsize = 16)\n",
    "    plt.title('Recent Innovations and the Early-Stage Funding Lag', fontsize = 20)\n",
    "    plt.legend()\n",
    "    \n",
    "    if saving_plots:\n",
    "        plt.savefig(\"Plots/Time-Series/Time_Difference_History_Summary.png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dates Based on T- Hype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_dates(keys, df_mains, df_return):\n",
    "    # Make a new columns with dates relative to the hype cycle\n",
    "    \n",
    "    for key in keys:\n",
    "        df_mains[key]['relative_date_days'] = df_mains[key]['date'] - (df_return[(df_return['Key']==key) & (df_return['Title']=='Earliest Moment of Peak Hype')]['Value']).iloc[0]\n",
    "        df_mains[key]['relative_date_days'] = df_mains[key]['relative_date_days'].dt.days\n",
    "        df_mains[key]['relative_date_weeks'] = df_mains[key]['relative_date_days']/7\n",
    "        df_mains[key]['relative_date_months'] = df_mains[key]['relative_date_days']/30.5\n",
    "        df_mains[key]['relative_date_years'] = df_mains[key]['relative_date_days']/365\n",
    "\n",
    "        \n",
    "    return df_mains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_dates_df(key, df, df_return):\n",
    "    # Make a new columns with dates relative to the hype cycle\n",
    "    \n",
    "    df['relative_date_days'] = df['Announced Date'] - (df_return[(df_return['Key']==key) & (df_return['Title']=='Earliest Moment of Peak Hype')]['Value']).iloc[0]\n",
    "    df['relative_date_days'] = df['relative_date_days'].dt.days\n",
    "    df['relative_date_weeks'] = df['relative_date_days']/7\n",
    "    df['relative_date_months'] = df['relative_date_days']/30.5\n",
    "    df['relative_date_years'] = df['relative_date_days']/365\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_dates_dfs(keys, dfs, df_return):\n",
    "        \n",
    "    # Make a new columns with dates relative to the hype cycle\n",
    "    count = 0\n",
    "    \n",
    "    for key in keys:\n",
    "        if count == 0:\n",
    "            df = dfs[key][1].copy(deep=True)\n",
    "\n",
    "            df['relative_date_days'] = df['Announced Date'] - (df_return[(df_return['Key']==key) & (df_return['Title']=='Earliest Moment of Peak Hype')]['Value']).iloc[0]\n",
    "            df['relative_date_days'] = df['relative_date_days'].dt.days\n",
    "            df['relative_date_weeks'] = df['relative_date_days']/7\n",
    "            df['relative_date_months'] = df['relative_date_days']/30.5\n",
    "            df['relative_date_years'] = df['relative_date_days']/365\n",
    "\n",
    "            df['Technology'] = key\n",
    "        else:\n",
    "            df_temp = dfs[key][1].copy(deep=True)\n",
    "\n",
    "            df_temp['relative_date_days'] = df_temp['Announced Date'] - (df_return[(df_return['Key']==key) & (df_return['Title']=='Earliest Moment of Peak Hype')]['Value']).iloc[0]\n",
    "            df_temp['relative_date_days'] = df_temp['relative_date_days'].dt.days\n",
    "            df_temp['relative_date_weeks'] = df_temp['relative_date_days']/7\n",
    "            df_temp['relative_date_months'] = df_temp['relative_date_days']/30.5\n",
    "            df_temp['relative_date_years'] = df_temp['relative_date_days']/365\n",
    "\n",
    "            df_temp['Technology'] = key            \n",
    "                        \n",
    "            df = df.append(df_temp, ignore_index=True)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "        # NEED TO CONVERT EACH $ TO A DATAPOINT\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
